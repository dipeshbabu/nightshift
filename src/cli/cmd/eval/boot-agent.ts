import { join, relative } from "path";
import { mkdirSync, existsSync, rmSync, symlinkSync, chmodSync, unlinkSync } from "fs";
import { createOpencodeClient } from "@opencode-ai/sdk/v2";
import OpenAI from "openai";
import {
  detectPlatform,
  pythonUrl,
  uvUrl,
  ripgrepUrl,
  opencodeUrl,
  buildXdgEnv,
  buildPath,
  waitForServer,
} from "../../../index";

type BootEvalStruct = {
  inputPath: string;
  model?: string;
  output: {
    skillsPaths: Array<string>;
    agentsMdPath: string;
  }
}

export type FileResult = {
  file: string;
  type: "skill" | "agentsMd";
  llmScores?: Record<string, number>;
  found: boolean;
  content: string | null;
}

export type BootEvalResult = {
  files: FileResult[];
}

type ProviderConfig = {
  name: string;
  envVar: string;
}

const PROVIDER_CONFIGS: Record<string, ProviderConfig> = {
  anthropic: { name: "Anthropic", envVar: "ANTHROPIC_API_KEY" },
  openai: { name: "OpenAI", envVar: "OPENAI_API_KEY" },
};

function detectProvider(model: string): ProviderConfig | null {
  const provider = model.split("/")[0]?.toLowerCase();
  return PROVIDER_CONFIGS[provider] ?? null;
}

async function promptForApiKey(provider: ProviderConfig): Promise<string> {
  process.stdout.write(`Enter your ${provider.name} API key: `);

  return new Promise((resolve) => {
    let input = "";
    process.stdin.setRawMode?.(true);
    process.stdin.resume();
    process.stdin.setEncoding("utf8");

    const onData = (char: string) => {
      if (char === "\n" || char === "\r") {
        process.stdin.setRawMode?.(false);
        process.stdin.pause();
        process.stdin.removeListener("data", onData);
        process.stdout.write("\n");
        resolve(input);
      } else if (char === "\u0003") {
        // Ctrl+C
        process.exit(1);
      } else if (char === "\u007f") {
        // Backspace
        if (input.length > 0) {
          input = input.slice(0, -1);
          process.stdout.write("\b \b");
        }
      } else {
        input += char;
        process.stdout.write("*");
      }
    };

    process.stdin.on("data", onData);
  });
}

type ApiKeyResult = {
  providerId: string;
  apiKey: string;
} | null;

async function ensureApiKey(model?: string): Promise<ApiKeyResult> {
  if (!model) return null;

  const provider = detectProvider(model);
  if (!provider) return null;

  const providerId = model.split("/")[0].toLowerCase();

  const existingKey = process.env[provider.envVar];
  if (existingKey) {
    console.log(`Using ${provider.name} API key from ${provider.envVar}`);
    return { providerId, apiKey: existingKey };
  }

  console.log(`No ${provider.envVar} found in environment.`);
  const apiKey = await promptForApiKey(provider);

  if (!apiKey.trim()) {
    throw new Error(`${provider.name} API key is required for model: ${model}`);
  }

  return { providerId, apiKey: apiKey.trim() };
}

const skillGraderPrompt = (content: string) => `
You are a strict, calibrated judge evaluating a SKILL.md file generated by a boot agent.

Be harsh. A score of 15+ on any dimension means genuinely excellent work. A score of 10 means adequate. Do not grade on formatting or length — grade on substance.

## SKILL.md Content to Evaluate

${content}

---

## Grading Dimensions (0–20 points each)

**D1 — Knowledge Delta (0–20)**
- 16–20: Contains genuine expert knowledge Claude cannot derive from first principles — decision trees, non-obvious trade-offs, domain-specific heuristics, and hard-won patterns.
- 11–15: Mix of expert knowledge and commonly known information; some genuinely useful insights buried among filler.
- 6–10: Mostly restates what models already know with occasional useful specifics.
- 0–5: Pure redundancy — "what is X" explanations, generic best practices, nothing an LLM couldn't generate unprompted.

**D2 — Specificity & Actionability (0–20)**
- 16–20: Every instruction is concrete and executable — exact commands are copy-pasteable, code examples compile, file paths are real, and an agent can follow them without interpretation or guesswork.
- 11–15: Most instructions are actionable but some steps require the agent to fill in gaps or interpret vague phrasing.
- 6–10: Mixes actionable commands with vague directives like "ensure quality" or "follow best practices."
- 0–5: Almost entirely abstract prose with no executable instructions.

**D3 — Anti-Patterns & Safety Boundaries (0–20)**
- 16–20: Defines explicit NEVER/ALWAYS/ASK-FIRST rules grounded in hard-won experience, each with a concrete reason — an agent reading this would avoid the specific mistakes that cause real damage in this domain.
- 11–15: Has some specific anti-patterns but missing reasoning, or covers obvious cases while missing the non-obvious dangerous ones.
- 6–10: Generic warnings like "be careful" without naming specific failure modes.
- 0–5: No anti-patterns or safety boundaries mentioned at all.

**D4 — Structure & Discoverability (0–20)**
- 16–20: The description field alone tells an agent exactly what this skill does and when to activate it; content is organized with progressive disclosure (critical decisions first, reference material later) and stays concise enough to fit in context without truncation.
- 11–15: Reasonable structure but the description is too vague to trigger correctly, or key information isn't surfaced early enough.
- 6–10: Flat structure with no clear hierarchy; an agent would have to read the entire document to find what it needs.
- 0–5: No description, no structure, wall of text.

**D5 — Tailoring to User Intent (0–20)**
- 16–20: Every section is customized to the user's stated purpose, chosen tech stack, and actual project structure — the skill reflects genuine understanding of the bootstrapped environment.
- 11–15: Partially adapted to the user intent but contains generic sections that could apply to any project.
- 6–10: Superficially references the user intent but the substance is generic.
- 0–5: Shows no adaptation to the specific user intent, installed packages, or library structure.

---

## Output Format

You MUST respond with ONLY the following JSON object, no other text:

{
  "score": <0-100 integer>,
  "dimensions": [
    { "dimension": "D1", "name": "Knowledge Delta", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D2", "name": "Specificity & Actionability", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D3", "name": "Anti-Patterns & Safety Boundaries", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D4", "name": "Structure & Discoverability", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D5", "name": "Tailoring to User Intent", "score": <0-20>, "reasoning": "<1-2 sentences>" }
  ]
}

The score MUST equal the sum of the five dimension scores. Each reasoning field must cite specific evidence from the generated output — never say "good" or "bad" without pointing to what you saw.`.trim();

const agentsMdGraderPrompt = (content: string) => `
You are a strict, calibrated judge evaluating an AGENTS.md file generated by a boot agent.

Be harsh. A score of 15+ on any dimension means genuinely excellent work. A score of 10 means adequate. Do not grade on formatting or length — grade on substance.

## AGENTS.md Content to Evaluate

${content}

---

## Grading Dimensions (0–20 points each)

**D1 — Project Specificity (0–20)**
- 16–20: Every section is grounded in the actual workspace — real package names, real file paths, real architecture decisions — the file could only belong to this project and no other.
- 11–15: References real project details but pads with generic advice that could apply anywhere.
- 6–10: Mostly generic with a few project-specific references sprinkled in.
- 0–5: A template with no project-specific content.

**D2 — Command Accuracy (0–20)**
- 16–20: Build, test, lint, and run commands are exact, copy-pasteable, and verified to work in this environment — an agent can execute them without modification or interpretation.
- 11–15: Most commands look correct but some may have wrong flags, missing arguments, or reference tools that weren't installed.
- 6–10: Commands are present but aspirational ("run the tests") rather than exact.
- 0–5: No commands section, or commands are clearly wrong.

**D3 — Safety Boundaries (0–20)**
- 16–20: Three clear tiers (ALWAYS do / ASK FIRST / NEVER do) name specific commands, files, and operations that matter for this project — an agent knows exactly where the guardrails are.
- 11–15: Has safety boundaries but missing a tier, or the rules are somewhat generic rather than project-specific.
- 6–10: Generic platitudes like "don't delete important files" without naming specifics.
- 0–5: No safety boundaries mentioned.

**D4 — Code Style Concreteness (0–20)**
- 16–20: Formatting rules and design patterns are shown through short code examples — an agent can pattern-match against them without interpreting prose descriptions of style.
- 11–15: Some code examples but also relies on prose descriptions for key style rules.
- 6–10: Mostly prose ("use clean architecture," "follow SOLID") with minimal or no code examples.
- 0–5: No style section, or purely abstract descriptions.

**D5 — Skill Catalog & Routing (0–20)**
- 16–20: Every available skill is listed with its exact name, file path, and a one-line description of when to use it — an agent can scan the list and know which skill to invoke for any given task without guessing or searching the filesystem.
- 11–15: Skills are listed but missing file paths, or descriptions are too vague to enable routing decisions.
- 6–10: Skills are mentioned by name only without paths or descriptions of when to use them.
- 0–5: Skills are not mentioned at all in the AGENTS.md.

---

## Output Format

You MUST respond with ONLY the following JSON object, no other text:

{
  "score": <0-100 integer>,
  "dimensions": [
    { "dimension": "D1", "name": "Project Specificity", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D2", "name": "Command Accuracy", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D3", "name": "Safety Boundaries", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D4", "name": "Code Style Concreteness", "score": <0-20>, "reasoning": "<1-2 sentences>" },
    { "dimension": "D5", "name": "Skill Catalog & Routing", "score": <0-20>, "reasoning": "<1-2 sentences>" }
  ]
}

The score MUST equal the sum of the five dimension scores. Each reasoning field must cite specific evidence from the generated output — never say "good" or "bad" without pointing to what you saw.`.trim();

async function gradeFile(fileResult: FileResult): Promise<Record<string, number>> {
  const openai = new OpenAI(); // Uses OPENAI_API_KEY from env

  const prompt = fileResult.type === "skill"
    ? skillGraderPrompt(fileResult.content!)
    : agentsMdGraderPrompt(fileResult.content!);

  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: "user", content: prompt }],
    response_format: { type: "json_object" },
  });

  // Parse JSON response and extract scores
  const content = response.choices[0]?.message?.content;
  if (!content) {
    throw new Error("No content in OpenAI response");
  }
  const result = JSON.parse(content);
  return {
    D1: result.dimensions[0].score,
    D2: result.dimensions[1].score,
    D3: result.dimensions[2].score,
    D4: result.dimensions[3].score,
    D5: result.dimensions[4].score,
    total: result.score,
  };
}

const bootPrompt = (userIntent: string) => `
You are bootstrapping a new workspace for the user.Their stated purpose is:
"${userIntent}"

## Important: Interview the User

If asked to read from a BOOT.md file, you must read it first.There will be information useful for you to help the user.

If there is a mention of Skills in either Markdown or JSON in the BOOT.md, you MUST use those to create skill files in your current working directory in the path.opencode / skills / <name>/SKILL.md.

NEVER ASK QUESTIONS

## After gathering information:

1. ** TODO project tracker **: You need to set up a TODO for this bootstrapping task so you don't forget any steps. Validate by reading back the TODOs and the environment.
1. ** Install packages **: Run \`uv add <packages>\` to install Python libraries appropriate for this use case
2. **Create library structure**: Add modules to src/agent_lib/ that will help with the stated purpose
3. **Generate SKILLS.md for each skill needed**: Create a SKILL.md at .opencode/skills/<name>/SKILL.md file with the SKILL.
3. **Generate AGENTS.md**: Create an AGENTS.md file following these best practices:

## SKILLS.md Guidelines:

You should always look up best practices for creating SKILLs.md files online before generating one. An extensive web search is recommended.

## AGENTS.md Guidelines:

You should always look up best practices for creating AGENTS.md files online before generating one. An extensive web search is recommended.

### Required Sections:
- **Project Overview**: One-sentence description tailored to "${userIntent}"
- **Commands**: Exact commands for build, test, run (use bun, uv, pytest)
- **Tech Stack**: Python 3.13, Bun, uv, and installed packages
- **Project Structure**: Key file paths and their purposes
- **Code Style**: Formatting rules, design patterns (use ruff, black)
- **Do's and Don'ts**: Specific, actionable guidelines for this use case
- **Safety Boundaries**:
  - Always do: Read files, run tests, format code
  - Ask first: Install new packages, modify pyproject.toml
  - Never do: Delete data, run destructive commands

### Style Guidelines:
- Be specific, not vague
- Use code examples, not descriptions
- Make commands copy-pasteable
- Prioritize capabilities over file structure
`.trim();

interface BinaryMapping {
  linkName: string;
  target: string;
}

async function installToolForEval(
  name: string,
  url: string,
  prefix: string,
  binaryMappings: BinaryMapping[],
): Promise<void> {
  const toolsDir = join(prefix, "tools", name);
  const binDir = join(prefix, "bin");
  mkdirSync(toolsDir, { recursive: true });
  mkdirSync(binDir, { recursive: true });

  const archiveName = url.split("/").pop()!;
  const archivePath = join(prefix, "tools", archiveName);

  // Download
  const downloadProc = Bun.spawn(["curl", "-fSL", "-o", archivePath, url], {
    stdout: "pipe",
    stderr: "pipe",
  });
  if ((await downloadProc.exited) !== 0) {
    throw new Error(`Failed to download ${url}`);
  }

  // Extract (handle both .tar.gz and .zip)
  if (archivePath.endsWith(".zip")) {
    const extractProc = Bun.spawn(["unzip", "-o", archivePath, "-d", toolsDir], {
      stdout: "pipe",
      stderr: "pipe",
    });
    if ((await extractProc.exited) !== 0) {
      throw new Error(`Failed to extract ${archiveName}`);
    }
  } else {
    const extractProc = Bun.spawn(["tar", "xf", archivePath, "-C", toolsDir], {
      stdout: "pipe",
      stderr: "pipe",
    });
    if ((await extractProc.exited) !== 0) {
      throw new Error(`Failed to extract ${archiveName}`);
    }
  }

  // Create symlinks
  for (const mapping of binaryMappings) {
    const linkPath = join(binDir, mapping.linkName);
    const targetAbsolute = join(toolsDir, mapping.target);

    if (!existsSync(targetAbsolute)) {
      console.warn(`  Warning: binary not found at ${targetAbsolute}`);
      continue;
    }

    chmodSync(targetAbsolute, 0o755);

    const relTarget = relative(binDir, targetAbsolute);
    if (existsSync(linkPath)) {
      unlinkSync(linkPath);
    }
    symlinkSync(relTarget, linkPath);
  }

  // Clean up archive
  unlinkSync(archivePath);
}

function generateEvalOpencodeConfig(model?: string): string {
  return JSON.stringify({
    "$schema": "https://opencode.ai/config.json",
    ...(model && { model }),
    "permission": {
      "edit": "allow",
      "bash": "allow",
      "webfetch": "allow",
      "write": "allow",
      "codesearch": "allow",
      "read": "allow",
      "grep": "allow",
      "glob": "allow",
      "list": "allow",
      "lsp": "allow",
      "skill": "allow",
      "todowrite": "allow",
      "todoread": "allow",
      "question": "allow"
    }
  }, null, 2);
}

async function installEvalEnvironment(prefix: string): Promise<void> {
  const platform = detectPlatform();

  console.log("Installing python...");
  const py = pythonUrl(platform);
  await installToolForEval("python", py.url, prefix, [
    { linkName: "python3", target: py.extractedBinary },
    { linkName: "python", target: py.extractedBinary },
  ]);

  console.log("Installing uv...");
  const uv = uvUrl(platform);
  await installToolForEval("uv", uv.url, prefix, [
    { linkName: "uv", target: uv.extractedBinary },
  ]);

  console.log("Installing ripgrep...");
  const rg = ripgrepUrl(platform);
  await installToolForEval("ripgrep", rg.url, prefix, [
    { linkName: "rg", target: rg.extractedBinary },
  ]);

  console.log("Installing opencode...");
  const oc = opencodeUrl(platform);
  await installToolForEval("opencode", oc.url, prefix, [
    { linkName: "opencode", target: oc.extractedBinary },
  ]);

  // Create XDG directories
  for (const dir of ["config", "share", "cache", "state"]) {
    mkdirSync(join(prefix, dir), { recursive: true });
  }
}

export async function bootEval(pathToTestFile: string): Promise<BootEvalResult> {
  debugger;
  // Read test struct and BOOT.md content
  const testStruct = await Bun.file(pathToTestFile).json() as BootEvalStruct;
  const bootMdContent = await Bun.file(testStruct.inputPath).text();
  const prompt = bootPrompt(bootMdContent);

  // Check for API key before starting (prompts user if needed)
  const authInfo = await ensureApiKey(testStruct.model);

  // Create temp directories
  const tempBase = `/tmp/booteval-${Date.now()}-${Math.random().toString(36).slice(2)}`;
  const prefix = join(tempBase, "prefix");
  const workspace = join(tempBase, "workspace");
  mkdirSync(prefix, { recursive: true });
  mkdirSync(workspace, { recursive: true });

  let serverProc: ReturnType<typeof Bun.spawn> | null = null;

  try {
    // Install tools into temp prefix
    console.log("Installing eval environment...");
    await installEvalEnvironment(prefix);

    // Write opencode.json to workspace
    await Bun.write(join(workspace, "opencode.json"), generateEvalOpencodeConfig(testStruct.model));

    // Start opencode server
    const port = 4096 + Math.floor(Math.random() * 1000);
    const serverUrl = `http://127.0.0.1:${port}`;
    const xdgEnv = buildXdgEnv(prefix);
    const opencodePath = join(prefix, "bin", "opencode");

    console.log(`Starting opencode server on port ${port}...`);
    serverProc = Bun.spawn([opencodePath, "serve", "--port", String(port)], {
      cwd: workspace,
      stdout: "pipe",
      stderr: "pipe",
      env: { ...process.env, ...xdgEnv, PATH: buildPath(prefix) },
    });

    // Wait for server
    await waitForServer(serverUrl);
    console.log("Server ready, starting eval...");

    // Create client and session
    const client = createOpencodeClient({ baseUrl: serverUrl });

    // Authenticate with API key if provided
    if (authInfo) {
      console.log(`Authenticating with ${authInfo.providerId}...`);
      await client.auth.set({
        providerID: authInfo.providerId,
        auth: { type: "api", key: authInfo.apiKey },
      });
      await client.instance.dispose();
      console.log("Authentication complete.");
    }

    const session = await client.session.create({ title: "Boot Eval" });
    if (!session.data) {
      throw new Error("Failed to create session");
    }
    const sessionId = session.data.id;

    // Set up event listener for completion
    const abort = new AbortController();
    const toolStates = new Map<string, string>();

    const sessionComplete = new Promise<void>((resolve, reject) => {
      (async () => {
        try {
          const events = await client.event.subscribe({}, { signal: abort.signal });
          for await (const event of events.stream) {
            // Only process events for our session
            const eventSessionId = event.properties?.sessionID ?? (event.properties as any)?.sessionID;
            if (eventSessionId && eventSessionId !== sessionId) continue;

            // Log text streaming
            if (event.type === "message.part.updated") {
              const { part, delta } = event.properties;
              if (part.type === "text" && delta) {
                process.stdout.write(delta);
              }
              // Log tool status changes
              if (part.type === "tool") {
                const prevState = toolStates.get(part.id);
                const currentState = part.state.status;
                if (prevState !== currentState) {
                  toolStates.set(part.id, currentState);
                  const title = (part.state as any).title || part.tool;
                  if (currentState === "running") {
                    console.log(`\n[Tool] Running: ${title}`);
                  } else if (currentState === "completed") {
                    console.log(`[Tool] Completed: ${title}`);
                  } else if (currentState === "error") {
                    const error = (part.state as any).error || "Unknown error";
                    console.log(`[Tool] Error: ${title} - ${error}`);
                  }
                }
              }
            }

            // Log permission requests (auto-approve for eval)
            if (event.type === "permission.asked") {
              const req = event.properties;
              console.log(`\n[Permission] Auto-approving: ${req.permission}`);
              await client.permission.reply({ requestID: req.id, reply: "once" });
            }

            // Session completed
            if (event.type === "session.idle" && event.properties.sessionID === sessionId) {
              console.log("\n[Session] Idle - completed");
              resolve();
              return;
            }

            // Session error
            if (event.type === "session.error" && (event.properties as any).sessionID === sessionId) {
              console.log(`\n[Session] Error: ${JSON.stringify(event.properties)}`);
              reject(new Error(`Session error: ${JSON.stringify(event.properties)}`));
              return;
            }
          }
        } catch (err) {
          if (!abort.signal.aborted) reject(err);
        }
      })();
    });

    // Send prompt
    await client.session.promptAsync({
      sessionID: sessionId,
      parts: [{ type: "text", text: prompt }],
    });

    // Wait with timeout
    const timeout = new Promise<void>((_, reject) => {
      setTimeout(() => reject(new Error("Boot eval timed out after 10 minutes")), 10 * 60 * 1000);
    });
    await Promise.race([sessionComplete, timeout]);
    abort.abort();

    // Collect output files
    const files: FileResult[] = [];

    for (const skillPath of testStruct.output.skillsPaths) {
      const fullPath = join(workspace, skillPath);
      const found = existsSync(fullPath);
      if (found) {
        const content = await Bun.file(fullPath).text();
        files.push({ file: skillPath, type: "skill", found: true, content });
      } else {
        files.push({ file: skillPath, type: "skill", found: false, content: null });
      }
    }

    const agentsMdFullPath = join(workspace, testStruct.output.agentsMdPath);
    const agentsMdFound = existsSync(agentsMdFullPath);
    if (agentsMdFound) {
      const content = await Bun.file(agentsMdFullPath).text();
      files.push({ file: testStruct.output.agentsMdPath, type: "agentsMd", found: true, content });
    } else {
      files.push({ file: testStruct.output.agentsMdPath, type: "agentsMd", found: false, content: null });
    }

    // LLM-as-judge grading phase
    console.log("Starting LLM grading phase...");
    for (const fileResult of files) {
      if (!fileResult.found) {
        fileResult.llmScores = { D1: 0, D2: 0, D3: 0, D4: 0, D5: 0, total: 0 };
        console.log(`File not found: ${fileResult.file} - all scores set to 0`);
      } else {
        console.log(`Grading: ${fileResult.file}...`);
        fileResult.llmScores = await gradeFile(fileResult);
        console.log(`Graded ${fileResult.file}: total=${fileResult.llmScores.total}`);
      }
    }

    console.log("Boot eval complete.");
    console.log(`=== RESULTS ===`)
    for (const fileResult of files) {
      console.log(`File: ${fileResult.file}, Found: ${fileResult.found}, Score: ${fileResult.llmScores?.total ?? 'N/A'}`);
    }
    return { files };

  } finally {
    // Cleanup
    if (serverProc) serverProc.kill();
    rmSync(tempBase, { recursive: true, force: true });
  }
}
